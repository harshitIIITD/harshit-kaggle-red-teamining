# Ollama Configuration (local inference - no API key required)
# Ensure Ollama is running: ollama serve

# Optional: Override default Ollama configuration
# OLLAMA_BASE_URL=http://localhost:11434
# TARGET_MODEL=llama3
# JUDGE_MODEL=llama3
# MAX_CONCURRENCY=8
# BATCH_SIZE_PER_CATEGORY=250

# Optional: Bandit algorithm settings
# BANDIT_ALGO=thompson
# EXPLORE_BIAS=0.35

# Optional: Evaluation thresholds
# JUDGE_THRESHOLD=0.72
# NOVELTY_THRESHOLD=0.65

# Optional: Storage paths
# SQLITE_PATH=data/state.db
# TRANSCRIPTS_PATH=data/attempts.jsonl
# FINDINGS_PATH=data/findings.jsonl
# REPORTS_DIR=data/reports/